{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assessment(url):\n",
    "    fes_collection_api = \"https://w3id.org/FAIR_Evaluator/collections/6/evaluate\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"resource\": url,\n",
    "        \"executor\": \"mwurzb\",\n",
    "        \"title\": \"comp_doi_test\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(fes_collection_api, headers=headers, json=payload)\n",
    "    response_json = response.json()\n",
    "    eval_result = json.loads(response_json.get(\"evaluationResult\"))\n",
    "    global count\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    print(\"_________________________\")\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metricurl(url):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url=url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_and_scores(eval_result):\n",
    "    eval_result_keys = eval_result.keys()\n",
    "    results = []\n",
    "    for k in eval_result_keys:\n",
    "        metric_url = get_metricurl(k).json().get(\"test_of_metric\")\n",
    "        metric_abbrev = re.search(r'(?<=Gen2_FM_).*$', metric_url)\n",
    "        metric = metric_abbrev.group()\n",
    "\n",
    "        eval_result_val = eval_result.get(k)\n",
    "        key = \"http://semanticscience.org/resource/SIO_000300\"\n",
    "        metric_score = int(eval_result_val[0][key][0][\"@value\"])\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"metric\": metric,\n",
    "                \"score\": metric_score\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_from_result(assessment_result):\n",
    "    return [item.get('metric') for item in assessment_result]\n",
    "\n",
    "def get_scores_from_result(assessment_result):\n",
    "    return [int(item.get('score')) for item in assessment_result]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons via DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_comp_assessment_result(df):\n",
    "    df_exp = pd.DataFrame()\n",
    "    df_exp[\"comparison\"] = list(chain.from_iterable([[x]*22 for x in df[\"comparisons\"]]))\n",
    "    try:\n",
    "        df_exp[\"doi\"] = list(chain.from_iterable([[x]*22 for x in df[\"doi\"]]))\n",
    "    except KeyError:\n",
    "        pass\n",
    "    df_exp[\"metric\"] = list(chain.from_iterable([get_metrics_from_result(assessment_result=x) for x in df[\"assessment_result\"]]))\n",
    "    df_exp[\"score\"] = list(chain.from_iterable([get_scores_from_result(assessment_result=x) for x in df[\"assessment_result\"]]))\n",
    "    df_exp = df_exp[[\"comparison\", \"doi\", \"metric\", \"score\"]]\n",
    "    return df_exp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons assessment via DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "comp_doi_df = pd.read_csv(\"raw_data/comparison_query_result_2023-03-29.csv\")\n",
    "comp_doi_df = comp_doi_df[comp_doi_df[\"doi\"].isna() == False].reset_index(drop=True)\n",
    "count = 0\n",
    "comp_doi_df[\"assessment_result\"] = comp_doi_df[\"doi\"].map(lambda x: get_assessment(url=\"https://doi.org/\" + x))\n",
    "comp_doi_df[\"assessment_result\"] = comp_doi_df[\"assessment_result\"].map(lambda x: get_metrics_and_scores(eval_result=x))\n",
    "comp_doi_df.to_csv(\"assessed_data/comparison_doi_df_FES_assessment.csv\")\n",
    "comp_doi_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons evaluation via DOI + visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparison</th>\n",
       "      <th>doi</th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://orkg.org/orkg/resource/R140347</td>\n",
       "      <td>10.48366/r140347</td>\n",
       "      <td>F1A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://orkg.org/orkg/resource/R140347</td>\n",
       "      <td>10.48366/r140347</td>\n",
       "      <td>F1B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://orkg.org/orkg/resource/R140347</td>\n",
       "      <td>10.48366/r140347</td>\n",
       "      <td>F1B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://orkg.org/orkg/resource/R140347</td>\n",
       "      <td>10.48366/r140347</td>\n",
       "      <td>F2A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://orkg.org/orkg/resource/R140347</td>\n",
       "      <td>10.48366/r140347</td>\n",
       "      <td>F2B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6463</th>\n",
       "      <td>http://orkg.org/orkg/resource/R576876</td>\n",
       "      <td>10.48366/r576876</td>\n",
       "      <td>I2A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6464</th>\n",
       "      <td>http://orkg.org/orkg/resource/R576876</td>\n",
       "      <td>10.48366/r576876</td>\n",
       "      <td>I2B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6465</th>\n",
       "      <td>http://orkg.org/orkg/resource/R576876</td>\n",
       "      <td>10.48366/r576876</td>\n",
       "      <td>I3A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466</th>\n",
       "      <td>http://orkg.org/orkg/resource/R576876</td>\n",
       "      <td>10.48366/r576876</td>\n",
       "      <td>R1.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6467</th>\n",
       "      <td>http://orkg.org/orkg/resource/R576876</td>\n",
       "      <td>10.48366/r576876</td>\n",
       "      <td>R1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6468 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 comparison               doi metric  score\n",
       "0     http://orkg.org/orkg/resource/R140347  10.48366/r140347    F1A      1\n",
       "1     http://orkg.org/orkg/resource/R140347  10.48366/r140347    F1B      1\n",
       "2     http://orkg.org/orkg/resource/R140347  10.48366/r140347    F1B      0\n",
       "3     http://orkg.org/orkg/resource/R140347  10.48366/r140347    F2A      1\n",
       "4     http://orkg.org/orkg/resource/R140347  10.48366/r140347    F2B      1\n",
       "...                                     ...               ...    ...    ...\n",
       "6463  http://orkg.org/orkg/resource/R576876  10.48366/r576876    I2A      1\n",
       "6464  http://orkg.org/orkg/resource/R576876  10.48366/r576876    I2B      0\n",
       "6465  http://orkg.org/orkg/resource/R576876  10.48366/r576876    I3A      1\n",
       "6466  http://orkg.org/orkg/resource/R576876  10.48366/r576876   R1.1      0\n",
       "6467  http://orkg.org/orkg/resource/R576876  10.48366/r576876   R1.1      1\n",
       "\n",
       "[6468 rows x 4 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_doi_df = pd.read_csv(\"assessed_data/comparison_doi_df_FES_assessment.csv\", index_col=0)\n",
    "comp_doi_df[\"assessment_result\"] = comp_doi_df[\"assessment_result\"].map(lambda x: x.replace(\"\\'\", \"\\\"\"))\n",
    "comp_doi_df[\"assessment_result\"] = comp_doi_df[\"assessment_result\"].map(lambda x: json.loads(x))\n",
    "comp_doi_df_assessed = expand_comp_assessment_result(comp_doi_df)\n",
    "comp_doi_df_assessed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons via ORKG URL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
